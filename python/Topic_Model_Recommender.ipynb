{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Based Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topic Based Recommender\n",
    "1. Represent articles in terms of Topic Vector\n",
    "2. Represent user in terms of Topic Vector of read articles\n",
    "3. Calculate cosine similarity between read and unread articles \n",
    "4. Get the recommended articles "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing parameters**:\n",
    "\n",
    "*1. PATH_ARTICLE_TOPIC_DISTRIBUTION: specify the path where 'ARTICLE_TOPIC_DISTRIBUTION.csv' is present.* <br/>\n",
    "*2. PATH_NEWS_ARTICLES: specify the path where news_article.csv is present*  <br/>\n",
    "*3. NO_OF_TOPIC: Number of topics specified when training your topic model. This would refer to the dimension of        each vector representing an article*  <br/>\n",
    "*4. ARTICLES_READ: List of Article_Ids read by the user*  <br/>\n",
    "*5. NO_RECOMMENDED_ARTICLES: Refers to the number of recommended articles as a result*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "PATH_ARTICLE_TOPIC_DISTRIBUTION = \"Article_Topic_Distribution.csv\"\n",
    "PATH_NEWS_ARTICLES = \"news_articles.csv\"\n",
    "NO_OF_TOPICS=150\n",
    "ARTICLES_READ=[7,6,76,61,761]\n",
    "NUM_RECOMMENDED_ARTICLES=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Represent Read Article in terms of Topic Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22186, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_distribution = pd.read_csv(PATH_ARTICLE_TOPIC_DISTRIBUTION)\n",
    "article_topic_distribution.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article_Id</th>\n",
       "      <th>Topic_Id</th>\n",
       "      <th>Topic_Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>0.324485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>0.131476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>127</td>\n",
       "      <td>0.535940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0.306691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>0.277037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Article_Id  Topic_Id  Topic_Weight\n",
       "0           0        25      0.324485\n",
       "1           0        27      0.131476\n",
       "2           0       127      0.535940\n",
       "3           1         5      0.306691\n",
       "4           1        47      0.277037"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_distribution.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Generate Article-Topic Distribution matrix ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4831, 150)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pivot the dataframe\n",
    "article_topic_pivot = article_topic_distribution.pivot(index='Article_Id', columns='Topic_Id', values='Topic_Weight')\n",
    "#Fill NaN with 0\n",
    "article_topic_pivot.fillna(value=0, inplace=True)\n",
    "#Get the values in dataframe as matrix\n",
    "articles_topic_matrix = article_topic_pivot.values\n",
    "articles_topic_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Topic_Id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>140</th>\n",
       "      <th>141</th>\n",
       "      <th>142</th>\n",
       "      <th>143</th>\n",
       "      <th>144</th>\n",
       "      <th>145</th>\n",
       "      <th>146</th>\n",
       "      <th>147</th>\n",
       "      <th>148</th>\n",
       "      <th>149</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Article_Id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.306691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.015589</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.077002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.396528</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Topic_Id    0    1    2         3    4         5    6    7    8    9   ...   \\\n",
       "Article_Id                                                             ...    \n",
       "0           0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0 ...    \n",
       "1           0.0  0.0  0.0  0.000000  0.0  0.306691  0.0  0.0  0.0  0.0 ...    \n",
       "2           0.0  0.0  0.0  0.000000  0.0  0.000000  0.0  0.0  0.0  0.0 ...    \n",
       "3           0.0  0.0  0.0  0.015589  0.0  0.077002  0.0  0.0  0.0  0.0 ...    \n",
       "4           0.0  0.0  0.0  0.000000  0.0  0.396528  0.0  0.0  0.0  0.0 ...    \n",
       "\n",
       "Topic_Id    140  141  142  143  144  145  146  147  148  149  \n",
       "Article_Id                                                    \n",
       "0           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "1           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "2           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "3           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "4           0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_topic_pivot.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Represent user in terms of Topic Vector of read articles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***A user vector is represented in terms of average of read articles topic vector***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select user in terms of read article topic distribution\n",
    "row_idx = np.array(ARTICLES_READ)\n",
    "read_articles_topic_matrix=articles_topic_matrix[row_idx[:, None]]\n",
    "#Calculate the average of read articles topic vector \n",
    "user_vector = np.mean(read_articles_topic_matrix, axis=0)\n",
    "user_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.02488209,  0.06438433,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.02753025,\n",
       "         0.        ,  0.18989699,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.04683422,\n",
       "         0.        ,  0.06889868,  0.        ,  0.00411056,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00662661,\n",
       "         0.        ,  0.        ,  0.09912603,  0.        ,  0.        ,\n",
       "         0.01028336,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00661727,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.05856521,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.04954107,  0.01280254,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00393764,  0.        ,\n",
       "         0.        ,  0.03582032,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.07245383,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.08082968,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.12301701,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Calculate cosine similarity between read and unread articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cosine_similarity(articles_topic_matrix, user_vector):\n",
    "    articles_similarity_score=cosine_similarity(articles_topic_matrix, user_vector)\n",
    "    recommended_articles_id = articles_similarity_score.flatten().argsort()[::-1]\n",
    "    #Remove read articles from recommendations\n",
    "    final_recommended_articles_id = [article_id for article_id in recommended_articles_id \n",
    "                                     if article_id not in ARTICLES_READ ][:NUM_RECOMMENDED_ARTICLES]\n",
    "    return final_recommended_articles_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[864, 2150, 2450, 629, 3643]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles_id = calculate_cosine_similarity(articles_topic_matrix, user_vector)\n",
    "recommended_articles_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Recommendation Using Topic Model:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Read\n",
      "6      Infosys shares likely to fall on Tuesday after...\n",
      "7      Dialogue crucial in finding permanent solution...\n",
      "61     Revathy to direct Queen s Tamil  Telugu remake...\n",
      "76     When cricketer R Ashwin started fans club for ...\n",
      "761     Baahubali  to have world television premiere ...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommender \n",
      "629      Dilwale  review roundup  What critics have to...\n",
      "864     Shah Rukh Khan-Kajol appear on Vijay TV show  ...\n",
      "2150    Year 2014 for Aamir Khan  Shah Rukh Khan and S...\n",
      "2450    Times Celebex  Akshay  Katrina top the list  S...\n",
      "3643    Will Aditya Chopra Bring Shah Rukh Khan and Ra...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Recommended Articles and their title\n",
    "news_articles = pd.read_csv(PATH_NEWS_ARTICLES)\n",
    "print 'Articles Read'\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(ARTICLES_READ)]['Title']\n",
    "print '\\n'\n",
    "print 'Recommender '\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(recommended_articles_id)]['Title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Topics + NER Recommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic + NER Based Recommender\n",
    "\n",
    "1. Represent user in terms of - <br/>\n",
    "        (Alpha) <Topic Vector> + (1-Alpha) <NER Vector> <br/>\n",
    "   where <br/>\n",
    "   Alpha => [0,1] <br/>\n",
    "   [Topic Vector] => Topic vector representation of concatenated read articles <br/>\n",
    "   [NER Vector]   => Topic vector representation of NERs associated with concatenated read articles <br/>\n",
    "2. Calculate cosine similarity between user vector and articles Topic matrix\n",
    "3. Get the recommended articles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALPHA = 0.5\n",
    "DICTIONARY_PATH = \"/home/phoenix/Documents/HandsOn/Final/python/Topic Model/model/dictionary_of_words.p\"\n",
    "LDA_MODEL_PATH = \"/home/phoenix/Documents/HandsOn/Final/python/Topic Model/model/lda.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import pickle\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Represent User in terms of Topic Distribution and NER\n",
    "\n",
    "1. Represent user in terms of read article topic distribution\n",
    "2. Represent user in terms of NERs associated with read articles\n",
    "        2.1 Get NERs of read articles\n",
    "        2.2 Load LDA model\n",
    "        2.3 Get topic distribution for the concated NERs\n",
    "3. Generate user vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Represent user in terms of read article topic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 150)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_idx = np.array(ARTICLES_READ)\n",
    "read_articles_topic_matrix=articles_topic_matrix[row_idx[:, None]]\n",
    "#Calculate the average of read articles topic vector \n",
    "user_topic_vector = np.mean(read_articles_topic_matrix, axis=0)\n",
    "user_topic_vector.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Represent user in terms of NERs associated with read articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get NERs of read articles\n",
    "def get_ner(article):\n",
    "    ne_tree = ne_chunk(pos_tag(word_tokenize(article)))\n",
    "    iob_tagged = tree2conlltags(ne_tree)\n",
    "    ner_token = ' '.join([token for token,pos,ner_tag in iob_tagged if not ner_tag==u'O']) #Discarding tokens with 'Other' tag\n",
    "    return ner_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NERs of Read Article => Narendra Modi Kashmir Modi Jammu Kashmir Modi Burhan Wani Omar Abdullah Abdullah National Conference Congress PCC CPI Tarigami Valley Modi Kashmir Jammu Kashmir Infosys Royal Bank Scotland RBS Williams Glyn Infosys IBM Infosys Application Delivery India Royal Bank Scotland Williams Glyn RBS Infosys Infosys Infosys Bombay Stock Infosys FY2017 Infosys YoY Cricketer Ravichandran Trisha Krishnan Ashwin Tamil Trisha Tamil Lesa Lesa Veteran Revathy Bollywood Queen Actress Suhasini Mani Ratnam Vikas Bahl Queen Paris Kangana Queen Telugu Tamil Filmmaker Thiagarajan Queen Telugu Tamil Revathy Suhasini Mani Ratnam Revathy Suhasini Mani Ratnam Suhasini Revathy Suhasini Mani Ratnam South Indian Telugu Suhasini Rajamouli Baahubali Malayalam Mazhavil Manorama Malayalam Prabhas Rana Daggubati Anushka Shetty Tamannaah Bhatia Baahubali Anushka Tamannaah Rana Prabhas Rajamouli Mazhavil Manorma Manorama Music Baahubali Malayalam VCD DVD Telugu MAA Baahubali Dussehra\n"
     ]
    }
   ],
   "source": [
    "articles = news_articles['Content'].tolist()\n",
    "user_articles_ner = ' '.join([get_ner(articles[i]) for i in ARTICLES_READ])\n",
    "print \"NERs of Read Article =>\", user_articles_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tknzr = TweetTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text=re.sub('[^\\w_\\s-]', ' ', text)                                            #remove punctuation marks \n",
    "    return cleaned_text                                                                    #and other symbols \n",
    "\n",
    "def tokenize(text):\n",
    "    word = tknzr.tokenize(text)                                                             #tokenization\n",
    "    filtered_sentence = [w for w in word if not w.lower() in stop_words]                    #removing stop words\n",
    "    stemmed_filtered_tokens = [stemmer.stem(plural) for plural in filtered_sentence]        #stemming\n",
    "    tokens = [i for i in stemmed_filtered_tokens if i.isalpha() and len(i) not in [0, 1]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning the article\n",
    "cleaned_text = clean_text(user_articles_ner)\n",
    "article_vocabulary = tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load model dictionary\n",
    "model_dictionary = pickle.load(open(DICTIONARY_PATH,\"rb\"))\n",
    "#Generate article maping using IDs associated with vocab\n",
    "corpus = [model_dictionary.doc2bow(text) for text in [article_vocabulary]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "lda =  models.LdaModel.load(LDA_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(9, 0.016833535075786221),\n",
       " (16, 0.13360130412473772),\n",
       " (21, 0.011354918964910036),\n",
       " (29, 0.048363063836432151),\n",
       " (31, 0.18383978754651545),\n",
       " (44, 0.016568883655345965),\n",
       " (84, 0.017429078934066415),\n",
       " (93, 0.041131451368969535),\n",
       " (106, 0.12909919386972013),\n",
       " (119, 0.20018375535066402),\n",
       " (127, 0.10765513761665123),\n",
       " (128, 0.036829724632851543),\n",
       " (145, 0.041718476547869268)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get topic distribution for the concated NERs\n",
    "article_topic_distribution=lda.get_document_topics(corpus[0])\n",
    "article_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_vector =[0]*NO_OF_TOPICS\n",
    "for topic_id, topic_weight in article_topic_distribution:\n",
    "    ner_vector[topic_id]=topic_weight\n",
    "user_ner_vector = np.asarray(ner_vector).reshape(1,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Generate user vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.        ,  0.        ,  0.01244104,  0.03219216,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.00841677,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01376513,\n",
       "         0.        ,  0.16174915,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.00567746,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.04759864,\n",
       "         0.        ,  0.12636923,  0.        ,  0.00205528,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.01159775,\n",
       "         0.        ,  0.        ,  0.04956302,  0.        ,  0.        ,\n",
       "         0.00514168,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00330864,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.03799714,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.04533626,  0.00640127,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.00196882,  0.        ,\n",
       "         0.        ,  0.08245976,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.13631879,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.09424241,  0.01841486,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.08236774,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha_topic_vector = ALPHA*user_topic_vector\n",
    "alpha_ner_vector = (1-ALPHA) * user_ner_vector\n",
    "user_vector = np.add(alpha_topic_vector,alpha_ner_vector)\n",
    "user_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Calculate cosine similarity between user vector and articles Topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1913, 2003, 1995, 1997, 864]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_articles_id = calculate_cosine_similarity(articles_topic_matrix, user_vector)\n",
    "recommended_articles_id\n",
    "# [array([ 0.75807146]), array([ 0.74644157]), array([ 0.74440326]), array([ 0.7420562]), array([ 0.73966259])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Get recommended articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles Read\n",
      "6      Infosys shares likely to fall on Tuesday after...\n",
      "7      Dialogue crucial in finding permanent solution...\n",
      "61     Revathy to direct Queen s Tamil  Telugu remake...\n",
      "76     When cricketer R Ashwin started fans club for ...\n",
      "761     Baahubali  to have world television premiere ...\n",
      "Name: Title, dtype: object\n",
      "\n",
      "\n",
      "Recommender \n",
      "864     Shah Rukh Khan-Kajol appear on Vijay TV show  ...\n",
      "1913    AIB Roast Controversy and Stringent Censor Boa...\n",
      "1995     Bajirao Mastani  Director Bhansali found AIB ...\n",
      "1997    Twinkle Khanna s Blog on AIB Roast Goes Viral ...\n",
      "2003    Deepika Padukone  Sonakshi Sinha  Alia Bhatt C...\n",
      "Name: Title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "#Recommended Articles and their title\n",
    "news_articles = pd.read_csv(PATH_NEWS_ARTICLES)\n",
    "print 'Articles Read'\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(ARTICLES_READ)]['Title']\n",
    "print '\\n'\n",
    "print 'Recommender '\n",
    "print news_articles.loc[news_articles['Article_Id'].isin(recommended_articles_id)]['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
