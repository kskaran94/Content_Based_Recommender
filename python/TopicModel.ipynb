{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "# Topic Modeling using LDA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling Using LDA\n",
    "\n",
    "1. Text Processing\n",
    "2. Generating dictionary of vocabulary\n",
    "3. Mapping corpus using dictionary\n",
    "4. Training and saving the Topic Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing parameters**:\n",
    "\n",
    "*1. PATH_NEWS_ARTICLES: specify the path where 'news_article.csv' is present* <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "PATH_NEWS_ARTICLES = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim import corpora, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. TEXT PROCESSING\n",
    "\n",
    "1.1 Clean the article - Remove punctuation marks, special characters <br/>\n",
    "1.2 Tokenize each article <br/>\n",
    "1.3 Stem each token <br/>\n",
    "1.4 Remove numberical tokens <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(PATH_NEWS_ARTICLES)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "tknzr = TweetTokenizer()\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text=re.sub('[^\\w_\\s-]', ' ', text)                                            #remove punctuation marks \n",
    "    return cleaned_text                                                                    #and other symbols \n",
    "\n",
    "def tokenize(text):\n",
    "    word = tknzr.tokenize(text)                                                             #tokenization\n",
    "    filtered_sentence = [w for w in word if not w.lower() in stop_words]                    #removing stop words\n",
    "    stemmed_filtered_tokens = [stemmer.stem(plural) for plural in filtered_sentence]        #stemming\n",
    "    tokens = [i for i in stemmed_filtered_tokens if i.isalpha() and len(i) not in [0, 1]]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleaning all articles\n",
    "# Returns a list containing list of words of each article\n",
    "def text_processing():\n",
    "    news_articles = df['Content'].tolist()\n",
    "    cleaned_text = list(map(clean_text, news_articles))\n",
    "    article_vocabulary = list(map(tokenize, cleaned_text))    \n",
    "    return article_vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_vocabulary = text_processing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 2. TOPIC MODELING\n",
    "\n",
    "2.1 Create dictionary mapping word to ID <br/>\n",
    "2.2 Map IDs to corpus <br/>\n",
    "2.3 Train LDA Model <br/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Parameters for LDA :-\n",
    "#NUMBER_OF_TOPICS is the number of requested latent topics to be extracted from the training corpus.\n",
    "NUMBER_OF_TOPICS = 5\n",
    "#PASSES refers to number of iterations\n",
    "PASSES = 1\n",
    "#NUMBER_OF_WORDS is the number of words for which you want to check the topic-word distribution\n",
    "NUMBER_OF_WORDS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Mapping vocabulary with IDs\n",
    "dictionary = corpora.Dictionary(article_vocabulary)\n",
    "pickle.dump(dictionary, open(\"dictionary_of_vocabulary.p\", \"wb\"))\n",
    "zip(dictionary.keys(),dictionary.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Mapping Vocabulary to Corpus\n",
    "corpus = [dictionary.doc2bow(text) for text in article_vocabulary]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Training LDA Model\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=NUMBER_OF_TOPICS,passes=PASSES) \n",
    "lda.save('lda.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Topic-Word Distribution\n",
    "topic_words = lda.show_topics(num_topics=NUMBER_OF_TOPICS, num_words=NUMBER_OF_WORDS)  # narray of Shape: n_topics*vocab\n",
    "topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Article - Topic Distribution for first Article\n",
    "def get_article_topic_distribution(article):\n",
    "    return lda.get_document_topics(article)\n",
    "#Returns a list containing a list of tuple\n",
    "#Each inner list corresponds to an article and each tuple refers to topicID and its corresponding probability  \n",
    "map(get_article_topic_distribution, corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Topics for a new Article\n",
    "*This is new article not used for training Topic Modeling*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_article = \"\"\"At the dawn of history India started on her unending quest, and trackless centuries are filled with her   \n",
    "              striving and the grandeur of her success and her failures. Through good and ill fortune alike she has  \n",
    "              never lost sight of that quest or forgotten the ideals which gave her strength. We end today a period of \n",
    "              ill fortune and India discovers herself again. The achievement we celebrate today is but a step, an opening \n",
    "              of opportunity, to the greater triumphs and achievements that await us. \n",
    "              Are we brave enough and wise enough to grasp this opportunity and accept the challenge of the future?\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Describing parameters**:\n",
    "\n",
    "*1. DICTIONARY_PATH: specify the path where 'dictionary_of_vocabulary.p' is present.* <br/>\n",
    "*2. LDA_MODEL_PATH: specify the path where 'lda.model' is present.* <br/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DICTIONARY_PATH = \"dictionary_of_vocabulary.p\"\n",
    "LDA_MODEL_PATH = \"lda.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cleaning the article\n",
    "cleaned_text = clean_text(new_article)\n",
    "article_vocabulary = tokenize(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load model dictionary\n",
    "model_dictionary = pickle.load(open(DICTIONARY_PATH,\"rb\"))\n",
    "#Generate article maping using IDs associated with vocab\n",
    "corpus = [model_dictionary.doc2bow(text) for text in [article_vocabulary]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Load LDA Model\n",
    "lda = models.LdaModel.load(LDA_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Article-Topic Distribution\n",
    "article_topic_distribution=lda.get_document_topics(corpus[0])\n",
    "article_topic_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
